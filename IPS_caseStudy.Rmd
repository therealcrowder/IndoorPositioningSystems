---
title: "Predicting Location via Indoor Positioning Systems"
author: "Mike Crowder, Brian Kolovich, Brandon Lawerance, Geardo Garza"
date: "6/2/2018"
output: pdf_document
---
<style>
body {
text-align: justify}
</style>

### Abstract
Fill in after writing the paper

### Introduction
This case was explored in detail in the book by Deborah Nolan and Duncan Temple Lang called "Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving". Indoor Positioning Systems (IPS) are often used because what we know as Global Positioning Systems (GPS) have a hard time working within buildings. Given the growth of wireless local area networks (LANs),IPS have the ability to use WiFi signals detected from network access points. Often questions about where is an object whether it be another person, yourself an object have the ability to be answered in real time.

### Background
Our [dataset](http://rdatasciencecases.org/Data/offline.final.trace.txt) is from a the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD). The "offline" is a referenced data set that houses signal strengths measured with a hand-held device on a grid of 166 points spaced 1 meter apart located in the hallways of a building at the University of Mannheim in Germany.

The floor plan measures 15 meters by 36 meters (49 feet by 118 feet). A floor plan is given in figure 1.

![Figure 1: Floor Plan of the Test Environment. *There are 6 fixed access points which are denoted by black square markers. The "offline" training data were collected at the locations marked with the grey dots. We can see that the grey dots are spaced a meter apart.*](/Users/mcrowder/Documents/Grad School/Quant World/IndoorPositioningSystemsPrediction/IPS/floorPlan.png)

Grey circles give us a reference to mark the locations in which the "offline" measurements were taken and the black squares mark six access points. The reference locations give us a calibration of the signal strengths for in the building. These locations will be used to build our model to predict the locations of our hand-held device when it's location is unknown.

The hand-held device provided us with x and y coordinates, much like that of latitude and longitude on a map. There is also an orientation of the device. Signal strengths are given at eight orientations in 45 degree increments (0, 45, 90 and so on). 110 signal strength measurements were recorded for each of the six access points for every location and orientation combination.

We had a couple of ways of setting up the data in this analysis, without getting into too much detail below is a table of the variables in the data set that we will use in the analysis.

Variable  | Description
------------- | -------------
t | timestamp in milliseconds since midnight, January 1, 1970 UTC
id | MAC address of the scanning device
pos | degree orientation of the user carrying the scanning device
MAC | MAC address of a responding peer (i.e. access point or a device in adhoc mode) with values for signal strength in dBm (Decibel-milliwatts)


### Libraries Required
If you don't have these libraries installed in your R environment please do so.
```{r}
library(codetools)
library(lattice)
library(fields)
```

#### Read in the data
```{r tidy = TRUE}
# Use URL to bring in text data
url <- "http://rdatasciencecases.org/Data/offline.final.trace.txt"
# Read in entire document
txt <- readLines(url)
# Each line in the offline file has been read into R as a string in the character vector txt
# Use the function substr() to locate lines/strings that start with '#' and count how many we have
sum(substr(txt, 1, 1) == "#")
length(txt)
```
With our "offline" data set there are 151,392 lines. The data documentation would tell us that we should expect there to be 146,080 lines (166 locations x 8 angles x 100 recordings). The difference between these two (151,392 and 146,080) is 5,312.

As a general rule it is better to check the entire data set instead of the first few lines.

#### Processing the Raw Data

Now that we have an idea of how to represent our data in R, we are now able to start the fun stuff. The data as is not in a format where we can simply use a function like read.table(). Our data are separated by semicolons. This gives us a basic structure in which we can process the data.

```{r tidy = TRUE}
strsplit(txt[4], ";")[[1]]
```

So, within these shorter strings, the "name" of the variable is separated by an '=' sign from the associated value. There are observations that contain multiple values where the separator is a ','. For example, "pos=0.0,0.0,0.0" consists of 3 position variables that are not named.

The vector, which is created by splitting on the semi-colon, and further split each element at the '=' sign is processed by splitting ','. We can create a function like the below:
```{r tidy = TRUE}
unlist(lapply(strsplit(txt[4], ";")[[1]],
  function(x)
    sapply(strsplit(x, "=")[[1]], strsplit, ",")))
```

We can make this more efficient by taking the "tokens" we created from above and form them into the appropriate form by using
```{r tidy=TRUE}
# create a spilt using ;,=,','
tokens <- strsplit(txt[4], "[;=,]")[[1]]
```

Let's look at the first 10 elements of our "tokens" variable to give us the information about the hand-held device. We can also extract the values of these variables.
```{r tidy=TRUE}
tokens[1:10]
# Extract values of variables
tokens[c(2, 4, 6:8, 10)]
```

From our data information we know that these variables correspond to the variables time, MAC address, *x,y,z* and orientation.

Take a look at the recorded signals within this observation. These are the remaining values in the split vector
```{r tidy=TRUE}
tokens[ - (1:10)]
```

These rows are a 4-column matrix or data frame giving the MAC address, signal, channel and device type. We need to detangle these and build a matrix. After the detanglement we can bind these columns with the values from the first 10 entries.
```{r tidy = TRUE}
tmp <- matrix(tokens[ - (1:10 ) ], ncol = 4, byrow = TRUE)
mat <- cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp),
  ncol = 6, byrow = TRUE), tmp)
# Check
dim(mat)
```

Now that we know that the above chunk of code works we can build a function to iterate through each row in the input file.
```{r tidy=TRUE}
processLine =
function(x)
{
  tokens = strsplit(x, "[;=,]")[[1]]
  tmp = matrix(tokens[ - (1:10) ], ncol = 4, byrow = TRUE)
  cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp),
               ncol = 6, byrow = TRUE), tmp)
}
```

Try to apply the function to several lines of the input:
```{r tidy=TRUE}
tmp = lapply(txt[4:20], processLine)
sapply(tmp, nrow)
```

Now that we have done the work of looking at the data, deciding the best way to break it down and the best way of separating it, we can now look at making this into a data frame. Do execute this we are going to use the do.call() function.
```{r tidy=TRUE}
offline = as.data.frame(do.call("rbind", tmp))
# Check it
dim(offline)
```

#### Validate Our Dataset

Work code through the entire data set
```{r tidy=TRUE}
lines <- txt[ substr(txt, 1, 1) != "#" ]
tmp = lapply(lines, processLine)
```

Well, lets dig to see what is going on here
```{r tidy=TRUE}
options(error = recover, warn = 2)
tmp = lapply(lines, processLine)
```

We will need to modify the function we made call ProcessLine(). We need to discard observations or add a single channel, and type. We will choose to remove these observations as they do not help us in developing our position system. Change the function to return NULL if the tokens vector only has 10 elements. The revised function becomes:
```{r tidy=TRUE}
processLine = function(x)
{
  tokens = strsplit(x, "[;=,]")[[1]]
  
  if (length(tokens) == 10) 
    return(NULL)
 
  tmp = matrix(tokens[ - (1:10) ], , 4, byrow = TRUE)
  cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow(tmp), 6, 
               byrow = TRUE), tmp)
}
```

Try again
```{r tidy=TRUE}
options(error = recover, warn = 1)
tmp <- lapply(lines, processLine)
offline <- as.data.frame(do.call("rbind", tmp), 
                        stringsAsFactors = FALSE)

dim(offline)
```

From the dim() function we can see we returned 1.18M rows. Our next step is convert our data into the proper data types. So we can do get to analysis and start looking at what this data is telling us.

#### Cleaning the Data for Analysis

First and foremost we need to name our variables. When naming variables, we have to make them meaningful.
```{r tidy=TRUE}
names(offline) = c("time", "scanMac", "posX", "posY", "posZ", 
                   "orientation", "mac", "signal", 
                   "channel", "type")
```

Now we covert the position, signal, and time variables to numeric.
```{r tidy=TRUE}
numVars = c("time", "posX", "posY", "posZ", 
            "orientation", "signal")
offline[ numVars ] =  lapply(offline[ numVars ], as.numeric)
```

The device could use a change to something that is more compreshensible than numbers 1 and 3. To facilitate this we can turn the variable into a factor with the levels, of "adhoc" and "access point". However, we will use only the signal strengths measured to the fix access points to develop and test our model. With this information now in hand we will remove records for adhoc measurements and remove the type variable from our data frame.
```{r tidy=TRUE}
offline = offline[ offline$type == "3", ]
offline = offline[ , "type" != names(offline) ]
dim(offline)
```

This removed over 100K observations from our data frame.

From here we now consider the variable time. From the docuementation, time is measured in the number of milliseconds from midnight on January 1st, 1970. We are able to scale the value of time to seconds and then simpy set the class of the time element in order to see the values as date-times in R. We will keep the more accurate time in rawTime in the case it is needed at a later time for analysis.
```{r tidy=TRUE}
offline$rawTime = offline$time
offline$time = offline$time/1000
class(offline$time) = c("POSIXt", "POSIXct")
```

```{r tidy=TRUE}
# Check what is going on
unlist(lapply(offline, class))
```

From the looks of it, it would appear that we have the right data types and structure, now lets go through another check and look at a summary of our data to see if our data makes sense. We are going to be looking for sane values in the descriptive statistics.
```{r tidy=TRUE}
summary(offline[, numVars])
```

So far, so good. Let's check the character variables.
```{r tidy=TRUE}
summary(sapply(offline[ , c("mac", "channel", "scanMac")], as.factor))
```

From what we can see from the above character summary we have a couple of items we need may need to modify.
  1.  We only have one value for scanMac. If you remember this is the MAC address for the hand-held device from which the measurement is taken. This variable will be discarded.
  2.  All of the values for posZ, the elevation of the hand-held device are 0. Why zero? Well we are just measuring the first floor. We can remove that variable as well.

We are now ready to start Exploratory Data Analysis (EDA)
```{r tidy=TRUE}
offline <- offline[ , !(names(offline) %in% c("scanMac", "posZ"))]
```

### Exploratory Data Analysis

#### Orientation

We have eight values for orientation, or we should. If the reader recalls the observations for orientation were set up to be at levels of 0 degrees, 45 degress, 90 degress and so on.
```{r tidy=TRUE}
length(unique(offline$orientation))
```

So, 203 is greater than 8. So we have more than eight values. We will take a closer look at the distribution of the variable orientation. We are going to do this by looking at an empirical cumulative distribution function, or ECDF.
```{r tidy=TRUE}
plot(ecdf(offline$orientation),
  main = "Orientation Distribution",
  xlab = "Orientation",
  ylab = "Empirical CDF")
```

Form the plot we do see observations clustered around 0, 45, 90 degress and so on, but we clearly have spread between. So, for example we have some 47.5 degress, 358.2 degrees and so on. This is not a loss, this information could be valuable as is. We could also gain value from creating a bin for these values to match the orignial eight values. To accomplish this we are going to create a function.
```{r tidy=TRUE}
roundOrientation = function(angles) {
  refs = seq(0, by = 45, length  = 9)
  q = sapply(angles, function(o) which.min(abs(o - refs)))
  c(refs[1:8], 0)[q]
}
# We now use our function to created the rounded angles
offline$angle <- roundOrientation(offline$orientation)
```

Let's continue our analysis of orientation with a box plot with the new angles.
```{r tidy=TRUE}
with(offline, boxplot(orientation ~ angle,
                      xlab = "nearest 45 degree angle",
                      ylab = "orientation"))
```

Our new angle variable worked, the outlier at the 0 degree at the top left are the values near 360 degrees that have been mapped to zero.

#### MAC Address Exploration

From the summary function above we observed that 126,529 occurrences of the address 00:14:br:3b:c7:c6, and the exact same number of occurrences of channel 2432000000. This would lead us to belive that there is one-to-one mapping. To prove this let's run a couple of test.
```{r tidy=TRUE}
c(length(unique(offline$mac)), length(unique(offline$channel)))
```

We find that there are 12 MAC addresses and eight channels. If we remember, there were only six access points. So why do see that there are eight channels and 12 MAC addresses? Re-reading the documentation we can see that there are additional access points that are not part of the testing area. These were not on the floor plan in figure 1. We need to check the counts of observations for the various MAC addresses.
```{r tidy=TRUE}
table(offline$mac)
```

So, we can see that the first, and last two MAC addresses are not close to the testing area. They could have not been working for the short amount of time during the measurement process. These addresses also don't have as high of counts.

The documenation tells us that the access points consist of five Linksys/Cisco and one Lancom L-54g routers. Using [MAC Address Lookup](http://coffer.com/mac_find/) we find that the MAC addresses that start with 00:14:bf belong to Linksys, the devices that start with 00:0f:a3 belong to Alpha Networks, and Lancom devices start with 00:a0:57. This means that there is a discrepancy with the documentation. With this known we will keep the top seven devices.
```{r tidy=TRUE}
subMacs <- names(sort(table(offline$mac), decreasing = TRUE))[1:7]
offline <- offline[ offline$mac %in% subMacs, ]
```

Let's validate by creating a table of counts for the remaining MACxchannel combinations and confirm there is one non-zero entry in each row.
```{r tidy=TRUE}
macChannel <- with(offline, table(mac, channel))
apply(macChannel, 1, function(x) sum(x > 0))
```

Now we do see that there is a one-to-one relationship between MAC address and channel for the seven devices. From here we can remove channel from offline.
```{r tidy=TRUE}
offline <- offline[ , "channel" != names(offline)]
```

#### Explore the Position of the Hand-Held Device

Finally, let's look at the position variables, posX and posY. How many different locations do we have data? Using the by() function we can add up the numbers of rows in our data for each unique (x,y) combination. This is initiated with by creating a data frame for each location.
```{r tidy=TRUE}
locDF <- with(offline,
            by(offline, list(posX, posY), function(x) x))
# Check
length(locDF)
```

So this list is longer than the number of combinations of actual (x,y) locations that were recorded. Let's check to see if we have missing values.
```{r tidy=TRUE}
sum(sapply(locDF, is.null))
```

Let's drop those combinations where nothing exists. Also we need to confirm how many values we are stuck with.
```{r tidy=TRUE}
locDF <- locDF[ !sapply(locDF, is.null) ]
# Check
length(locDF)
```

We can operate on each of these data frame to determine the number of obsercations recorded at each location with:
```{r tidy=TRUE}
locCounts <- sapply(locDF, nrow)

# If we want to keep the position information with the lcoation, we do this with
locCounts <- sapply(locDF,
                    function(df)
                      c(df[1, c("posX", "posY")], count = nrow(df)))
# Confirm we have a matrix
class(locCounts)
# Confirm 3 rows
dim(locCounts)
# Examine a few of the counts
locCounts[ , 1:8]
```

At each position we roughly have 5,500 recordings at each position. If we think about it, with 8 orientations X 110 replications X 7 access points, which is 6,100 signal strenght measurements.

We can visualize all 166 counts by adding the counts as text at their respective locations, changing the szize and angle of the characters to avoid overlapping text. We first transpose the matrix to keep the locations as columns we then make the plot with:
```{r tidy=TRUE}
pdf(file = "Geo_XYByCount.pdf", width = 10)
oldPar = par(mar = c(3.1, 3.1, 1, 1))

locCounts = t(locCounts)
plot(locCounts, type = "n", xlab = "", ylab = "")
text(locCounts, labels = locCounts[,3], cex = .8, srt = 45)

par(oldPar)
dev.off()
```

This picture in figure 2 should look familar. It looks just like our floorplan from figure 1. We can see that there are roughly the same number signals detected at each location.

![Figure 2: Counts of signals at each position. *Plotted at each location in the building is the total number of signals detected from all access points for the offline data. In a perfect world there is 110 signals measured with 8 angles for each 6 access points for a total of 5,280 recordings. These data include a 7th MAC address and not all signals were detected, so there are about 5,500 recordings at each location.*](/Users/mcrowder/Documents/Grad School/Quant World/IndoorPositioningSystemsPrediction/IPS/Geo_XYByCount.pdf)

We have examined all of the variables with exception of time and signal. The process has helped us clean our data and reduce it to those records that are releant to our analysis. We now leave the examination of the signals to the next section where we study the it's distributional properties. Time is not directly related to our model, but it does give us the order in which the observations were taken. In an expirement time could help uncover potential bias. We could encounter bias if the person carrying the hand-held device may have changed how the device was carried as the experiment progressed and this could have changed the strength of the signal.

We will need to read the online data in R. We will use a readData() function to accomplish this.
```{r tidy=TRUE}
readData = 
  function(filename = "http://rdatasciencecases.org/Data/offline.final.trace.txt", 
           subMacs = c("00:0f:a3:39:e1:c0", "00:0f:a3:39:dd:cd", "00:14:bf:b1:97:8a",
                       "00:14:bf:3b:c7:c6", "00:14:bf:b1:97:90", "00:14:bf:b1:97:8d",
                       "00:14:bf:b1:97:81"))
  {
    txt = readLines(filename)
    lines = txt[ substr(txt, 1, 1) != "#" ]
    tmp = lapply(lines, processLine)
    offline = as.data.frame(do.call("rbind", tmp), 
                            stringsAsFactors= FALSE) 
    
    names(offline) = c("time", "scanMac", 
                       "posX", "posY", "posZ", "orientation", 
                       "mac", "signal", "channel", "type")
    
     # keep only signals from access points
    offline = offline[ offline$type == "3", ]
    
    # drop scanMac, posZ, channel, and type - no info in them
    dropVars = c("scanMac", "posZ", "channel", "type")
    offline = offline[ , !( names(offline) %in% dropVars ) ]
    
    # drop more unwanted access points
    offline = offline[ offline$mac %in% subMacs, ]
    
    # convert numeric values
    numVars = c("time", "posX", "posY", "orientation", "signal")
    offline[ numVars ] = lapply(offline[ numVars ], as.numeric)

    # convert time to POSIX
    offline$rawTime = offline$time
    offline$time = offline$time/1000
    class(offline$time) = c("POSIXt", "POSIXct")
    
    # round orientations to nearest 45
    offline$angle = roundOrientation(offline$orientation)
      
    return(offline)
  }

offlineRedo = readData()
# Check to see if this fucntion matches what we did.
identical(offline, offlineRedo)
```

Confirm what variables are global
```{r tidy=TRUE}
findGlobals(readData, merge = FALSE)$variables
```

### Signal Strength Analysis

Using visualization and statistical summaries we now turn to investigating the response variable, signal strength. We are going to have to learn more about how the signals behave before designing a model for IPS. To do this we need to aks the following questions.

  * Signal strength has been measured to an access point multiple times at each location and orientation. So, how do these signal strengths behave? What is the distribution of the repeated measurements at each location and orientation? Does signal strength behave the same at all of the locations? Or, is this a case of location, orientation, and access points having an effect on the distribution?
  * In perfect conditions (lab setting), signal strength decays linearly with log distance and a simple triangulation using the signal strength from three access points can give an accurate pinpoint to the location of a device. The physical characteristics of a building and the activity in it can add a large amount of "noise" to signal strength measurements. Our questions become how do we describe the relationship between the signal strength and the distance from the device to the access point? How does orientation affect this relationship. Finally, is this relationship the same for all access points?
  
#### Distribution of Signal Strength

We need to compare the distribution of signal strengh at different orientations and for different access points. This is going to require us to subdvide our data. We are interested in seeing if these distributions are normal or skewed. We also want to look at the variances.

We will fix a location on the map to see the impact of orientation and signal strength, while the experimenter rotates through the eight angles. We will also measure the MAC addresses seperately because at an orientation of 90 degrees the experimenter may be facing toward one access point and away from another. We will check this with a boxplot.
```{r tidy=TRUE}
bwplot(signal ~ factor(angle) | mac, data = offline,
        subset = posX == 2 & posY == 12
                  & mac != "00:0f:a3:39:dd:cd",
        layout = c(2,3))
```

The signal strength does vary with the orientation for both close and distant access points. Recall from the summary section that signal strengths are measured in negative values.
```{r tidy=TRUE}
summary(offline$signal)
```

The small values (-98) are weaker signals, while the larger values are the stronger signals. When other locations are examined we see a simular dependence of signal strength on angle.
```{r tidy=TRUE}
pdf(file = "Geo_DensitySignalByMacAngle.pdf", width = 8, height = 12)
oldPar = par(mar = c(3.1, 3, 1, 1))

densityplot( ~ signal | mac + factor(angle), data = offline,
             subset = posX == 24 & posY == 4 & 
                         mac != "00:0f:a3:39:dd:cd",
             bw = 0.5, plot.points = FALSE)

par(oldPar)
dev.off()
```

Many of the distributions in figure 3 look normal, but there are some serious deviations with secondary modes and skewness. The center of the distribution does vary with the angle and MAC address. This would be indictive of a relationship.

Basically we would need thousands of boxplots to see all of the relationships, we don't have time for that. We will examine measures of center and measures of spread of signal strength for all location-orientation-access point combinations. For each combination we basically have around 100 observations. To compute summary statistics for these various combinations we need to make a factor that contains all of the unique combinations of the observed (x,y) pairs for the 166 locations.

![Figure 3: Distribution of Signal by Angle for Each Access Point. *The density curves shown for signal strengths are measured at the position x = 24 and y = 4. Many look normal, with others look like there is skew present*](/Users/mcrowder/Documents/Grad School/Quant World/IndoorPositioningSystemsPrediction/IPS/Geo_DensitySignalByMacAngle.pdf)

```{r tidy=TRUE}
offline$posXY <- paste(offline$posX, offline$posY, sep = "-")
```
Now, we create a list of data frames for every combination of (x, y), angle, and access point as follows
```{r tidy=TRUE}
byLocAngleAP <- with(offline, 
                    by(offline, list(posXY, angle, mac), 
                       function(x) x))
```

We should be able to calculate summary statistics on each of these data frames with
```{r tidy=TRUE}
signalSummary = 
  lapply(byLocAngleAP,            
         function(oneLoc) {
           ans = oneLoc[1, ]
           ans$medSignal = median(oneLoc$signal)
           ans$avgSignal = mean(oneLoc$signal)
           ans$num = length(oneLoc$signal)
           ans$sdSignal = sd(oneLoc$signal)
           ans$iqrSignal = IQR(oneLoc$signal)
           ans
           })

offlineSummary = do.call("rbind", signalSummary)
```

Now we take our results from above and take a visual look at the measures of spread with boxplots. Average signal will be converted to a categorical variable for this exercise.
```{r tidy=TRUE}
breaks = seq(-90, -30, by = 5)
bwplot(sdSignal ~ cut(avgSignal, breaks = breaks),
       data = offlineSummary, 
       subset = mac != "00:0f:a3:39:dd:cd",
       xlab = "Mean Signal", ylab = "SD Signal")
```

We see that weakest signals have the smaller standard deviation and it would appear that as signal strength increases so does the standard deviation. This is going to be an important concept going into modeling.

We can examine the skew of signal strength by plotting the difference avgSignal - medSignal, against the number of observations. We are going to use a smooth scatter to help with over plotting. We will also add a local average of the difference between the mean and the median to better help address the size.

```{r tidy=TRUE}
with(offlineSummary,
     smoothScatter((avgSignal - medSignal) ~ num,
                   xlab = "Number of Observations", 
                   ylab = "mean - median"))
abline(h = 0, col = "#984ea3", lwd = 2)

lo.obj = 
  with(offlineSummary,
       loess(diff ~ num, 
             data = data.frame(diff = (avgSignal - medSignal),
                               num = num)))

lo.obj.pr = predict(lo.obj, newdata = data.frame(num = (70:120)))
lines(x = 70:120, y = lo.obj.pr, col = "#4daf4a", lwd = 2)
```

The above comparison fo mean and median signal srength displays a smoothed scatter plot. The difference between the mean and meadian signal strength for each combination of location, access point and angle against the number of observations. The differences are close to zero with a typical deviation of 1 to 2 bBm.

Another plot we are going to try is with the use of predictSurface() to predict the value for the fitted surface at a grid of the observed posX and posY values with a contour plot. This is simular to what you see in a topographical map. The areas where we have a strong signal corresponds to the higher evelvated areas of a contour map. Just like in the last visualization we want to control for the access point and the orientation. We will begin by selecting one MAC address and one orientation to examine.
```{r tidy=TRUE}
oneAPAngle <- subset(offline, mac == subMacs[5] & angle == 0)
```
We can make a topographical map using color, just like a heatmap. The fields package uses the method of thin plate splines to fit a surface to the signal strength value at the observed locations. 
```{r tidy=TRUE}
oneAPAngle = subset(offlineSummary, 
                    mac == subMacs[5] & angle == 0)

smoothSS = Tps(oneAPAngle[, c("posX","posY")], 
               oneAPAngle$avgSignal)

vizSmooth = predictSurface(smoothSS)

plot.surface(vizSmooth, type = "C")

points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5)

surfaceSS = function(data, mac, angle = 45) {
  require(fields)
  oneAPAngle = data[ data$mac == mac & data$angle == angle, ]
  smoothSS = Tps(oneAPAngle[, c("posX","posY")], 
                 oneAPAngle$avgSignal)
  vizSmooth = predictSurface(smoothSS)
  plot.surface(vizSmooth, type = "C", 
               xlab = "", ylab = "", xaxt = "n", yaxt = "n")
  points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5) 
}

parCur = par(mfrow = c(2,2), mar = rep(1, 4))

mapply(surfaceSS, mac = subMacs[ rep(c(5, 1), each = 2) ], 
       angle = rep(c(0, 135), 2),
       data = list(data = offlineSummary))
 
par(parCur)
```

This figure is telling us that the location of the access point is the dark red region at the top of the "hill". We can also see the effect of the orientation on signel strength. In addition we can also detect a corridor effect. The signal is greater relative to the distance along the corridors where the signals are not blocked by walls.
